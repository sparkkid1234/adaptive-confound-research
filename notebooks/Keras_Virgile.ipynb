{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add path to our development module to sys.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/virgile/.conda/envs/py36/lib/python3.6/importlib/_bootstrap.py:205: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n",
      "/home/virgile/.conda/envs/py36/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from importlib import reload\n",
    "import adaptive_confound.utils as acu\n",
    "import adaptive_confound.control as acc\n",
    "import adaptive_confound.topic_model as actm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normal imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.sparse as sp\n",
    "import pickle\n",
    "import os.path\n",
    "import tensorflow as tf\n",
    "import itertools as it\n",
    "\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "from sklearn import metrics, preprocessing\n",
    "from copy import deepcopy\n",
    "from scipy.stats import mode\n",
    "from collections import Counter, defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "from keras.layers import Dense, Input, Lambda\n",
    "from keras.models import Model, Sequential\n",
    "from keras.callbacks import TensorBoard, EarlyStopping, ModelCheckpoint\n",
    "from keras.datasets import reuters\n",
    "from keras.losses import binary_crossentropy, mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.backend.tensorflow_backend import set_session\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.5\n",
    "sess = tf.Session(config=config)\n",
    "set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_colwidth = 200\n",
    "pd.options.display.max_columns = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Twitter gender/location data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ystr = 'location'\n",
    "zstr = 'gender'"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "tX = pd.read_pickle(\"/data/virgile/confound/adaptive/in/twitter_gender_location/term_doc_matrix.pkl\")\n",
    "tusers = pd.read_pickle(\"/data/virgile/confound/adaptive/in/twitter_gender_location/users_array.pkl\")\n",
    "tvec = pd.read_pickle(\"/data/virgile/confound/adaptive/in/twitter_gender_location/vectorizer.pkl\")\n",
    "\n",
    "tfeatures = np.array(tvec.get_feature_names())\n",
    "ty = np.array(sum([ [_[ystr] for _ in tusers[i]] for i in [0, 1] ], []))\n",
    "tley = LabelEncoder().fit(ty)\n",
    "ty_enc = tley.transform(ty)\n",
    "tz = np.array(sum([ [_[zstr] for _ in tusers[i]] for i in [0, 1] ], []))\n",
    "tlez = LabelEncoder().fit(tz)\n",
    "tz_enc = tlez.transform(tz)\n",
    "\n",
    "twitter = acu.Dataset(\n",
    "    X=tX,\n",
    "    y=ty_enc,\n",
    "    z=tz_enc,\n",
    "    features=tfeatures,\n",
    "    vectorizer=tvec,\n",
    "    le_z = tlez,\n",
    "    le_y = tley\n",
    ")\n",
    "twitter.to_pickle(\"/data/virgile/confound/adaptive/in/twitter_dataset_y={}_z={}.pkl\".format(ystr, zstr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter = acu.read_pickle(\"/data/virgile/confound/adaptive/in/twitter_dataset_y={}_z={}.pkl\".format(ystr, zstr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "twitter.create_biased_datasets(1000, np.linspace(.1,.9,9), k=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Yelp data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "yelp = acu.read_pickle(\"/data/virgile/confound/adaptive/in/balanced_yelp.pkl\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "yelp.create_biased_datasets(100, [.1,.2,.3,.4,.5,.6,.7,.8,.9], k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "< Dataset: size=14128, p(y)=0.50, p(z)=0.50, bias=0.50, parent=< Dataset: size=57342, p(y)=0.78, p(z)=0.73, bias=0.79, parent=None > >"
      ]
     },
     "execution_count": 339,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yelp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic models benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/81 [00:00<?, ?it/s]\u001b[A\n"
     ]
    }
   ],
   "source": [
    "n_topics = 100\n",
    "models = [\n",
    "    actm.DenseNeuralNet(k=twitter.features.size, m=n_topics),\n",
    "    actm.VariationalAutoEncoder(k=twitter.features.size, m=n_topics, \n",
    "                                model_params=dict(hidden_dim=n_topics, beta=1, supervised=False)),\n",
    "    actm.LatentDirichletAllocation(k=twitter.features.size, m=n_topics)\n",
    "]\n",
    "list_kwargs = [\n",
    "    dict(epochs=100, verbose=0),\n",
    "    dict(epochs=100, verbose=0),\n",
    "    dict()\n",
    "]\n",
    "\n",
    "product_dataset = it.product(\n",
    "    twitter.get_biased_datasets(kind='train'),\n",
    "    twitter.get_biased_datasets(kind='test')\n",
    ")\n",
    "iterator = tqdm( list( enumerate( product_dataset ) ) )\n",
    "savedir = \"/data/virgile/confound/adaptive/out/twitter/datasets_with_topic_distrib/{}\"\n",
    "\n",
    "for i, (d_train, d_test) in iterator:\n",
    "    full_d = acu.dataset.concat([d_train, d_test])\n",
    "    d_train.topics = {}\n",
    "    d_test.topics = {}\n",
    "    d_train_path = savedir.format(\"d_train_{:03d}.pkl\".format(i))\n",
    "    d_test_path = savedir.format(\"d_test_{:03d}.pkl\".format(i))\n",
    "    for tm, kwargs in zip(models, list_kwargs):\n",
    "        tm.fit(full_d, **kwargs)\n",
    "        tm_name = tm.get_name()\n",
    "        d_train.topics[tm_name] = tm.transform(d_train)\n",
    "        d_test.topics[tm_name] = tm.transform(d_test)\n",
    "    d_train.to_pickle(d_train_path, with_parent=False)\n",
    "    d_test.to_pickle(d_test_path, with_parent=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "results['f1'] = results.apply(lambda r: metrics.f1_score(r.y_true, r.y_pred), axis=1)\n",
    "results['acc'] = results.apply(lambda r: metrics.accuracy_score(r.y_true, r.y_pred), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "results['corr_diff'] = results.tr_corr - results.te_corr\n",
    "results['corr_diff_round'] = results.corr_diff.round(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "classifier          z_i  corr_diff_round  z_i\n",
       "BackdoorAdjustment  0    -1.6             0      0.712099\n",
       "                         -1.2             0      0.788196\n",
       "                         -0.8             0      0.822580\n",
       "                         -0.4             0      0.845131\n",
       "                          0.0             0      0.855025\n",
       "                          0.4             0      0.865866\n",
       "                          0.8             0      0.851304\n",
       "                          1.2             0      0.825668\n",
       "                          1.6             0      0.783084\n",
       "LogisticRegression  0    -1.6             0      0.622322\n",
       "                         -1.2             0      0.747925\n",
       "                         -0.8             0      0.820578\n",
       "                         -0.4             0      0.862361\n",
       "                          0.0             0      0.883075\n",
       "                          0.4             0      0.861958\n",
       "                          0.8             0      0.821431\n",
       "                          1.2             0      0.751994\n",
       "                          1.6             0      0.646421\n",
       "Name: f1, dtype: float64"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.groupby(['classifier', 'z_i', 'corr_diff_round', 'z_i']).f1.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>classifier</th>\n",
       "      <th>te_corr</th>\n",
       "      <th>topic_model</th>\n",
       "      <th>tr_corr</th>\n",
       "      <th>y_pred</th>\n",
       "      <th>y_true</th>\n",
       "      <th>z_i</th>\n",
       "      <th>f1</th>\n",
       "      <th>corr_diff</th>\n",
       "      <th>corr_diff_round</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BackdoorAdjustment</td>\n",
       "      <td>-0.822216</td>\n",
       "      <td>TrueTopic</td>\n",
       "      <td>-0.776347</td>\n",
       "      <td>[0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, ...</td>\n",
       "      <td>[0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.878728</td>\n",
       "      <td>0.045869</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>-0.822216</td>\n",
       "      <td>None</td>\n",
       "      <td>-0.776347</td>\n",
       "      <td>[0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, ...</td>\n",
       "      <td>[0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.875248</td>\n",
       "      <td>0.045869</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BackdoorAdjustment</td>\n",
       "      <td>-0.418157</td>\n",
       "      <td>TrueTopic</td>\n",
       "      <td>-0.776347</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.842002</td>\n",
       "      <td>-0.358190</td>\n",
       "      <td>-0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>-0.418157</td>\n",
       "      <td>None</td>\n",
       "      <td>-0.776347</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.823069</td>\n",
       "      <td>-0.358190</td>\n",
       "      <td>-0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BackdoorAdjustment</td>\n",
       "      <td>-0.016131</td>\n",
       "      <td>TrueTopic</td>\n",
       "      <td>-0.776347</td>\n",
       "      <td>[1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.795205</td>\n",
       "      <td>-0.760217</td>\n",
       "      <td>-0.8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           classifier   te_corr topic_model   tr_corr  \\\n",
       "0  BackdoorAdjustment -0.822216   TrueTopic -0.776347   \n",
       "1  LogisticRegression -0.822216        None -0.776347   \n",
       "2  BackdoorAdjustment -0.418157   TrueTopic -0.776347   \n",
       "3  LogisticRegression -0.418157        None -0.776347   \n",
       "4  BackdoorAdjustment -0.016131   TrueTopic -0.776347   \n",
       "\n",
       "                                                                                                                                                                                                    y_pred  \\\n",
       "0  [0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, ...   \n",
       "1  [0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, ...   \n",
       "2  [0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, ...   \n",
       "3  [0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, ...   \n",
       "4  [1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, ...   \n",
       "\n",
       "                                                                                                                                                                                                    y_true  \\\n",
       "0  [0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, ...   \n",
       "1  [0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, ...   \n",
       "2  [0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, ...   \n",
       "3  [0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, ...   \n",
       "4  [1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, ...   \n",
       "\n",
       "   z_i        f1  corr_diff  corr_diff_round  \n",
       "0    0  0.878728   0.045869              0.0  \n",
       "1    0  0.875248   0.045869              0.0  \n",
       "2    0  0.842002  -0.358190             -0.4  \n",
       "3    0  0.823069  -0.358190             -0.4  \n",
       "4    0  0.795205  -0.760217             -0.8  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.head()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "g = sns.FacetGrid(results, row='z_i', col='topic_model', hue='z_i')\n",
    "g.map(sns.pointplot, 'corr_diff_round', 'f1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.to_json(\"/data/virgile/confound/adaptive/out/twitter/dnn_vs_truetopic.jsonl\", lines=True, orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2084, 3316,  207, ...,  116,  133,  127], dtype=int64)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twitter.X.sum(axis=0).A1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_pxgxi(d):\n",
    "    n, k = d.X.shape\n",
    "    result = np.ones((k,k))\n",
    "    sum_fts = 1 + d.X.sum(axis=0).A1\n",
    "    for xi in tqdm(range(k)):\n",
    "        if sp.issparse(d.X):\n",
    "            keep = d.X[:, xi].nonzero()[0]\n",
    "        else:\n",
    "            keep = np.where(d.X[:, xi] != 0)[0]\n",
    "        result[xi] += d.X[keep].sum(axis=0).A1\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21981/21981 [05:32<00:00, 66.21it/s] \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 335.,  234.,   20., ...,    9.,   11.,   10.],\n",
       "       [ 234.,  565.,   38., ...,   14.,   17.,   17.],\n",
       "       [  20.,   38.,   41., ...,    3.,    3.,    2.],\n",
       "       ..., \n",
       "       [   9.,   14.,    3., ...,   21.,    4.,    1.],\n",
       "       [  11.,   17.,    3., ...,    4.,   22.,    1.],\n",
       "       [  10.,   17.,    2., ...,    1.,    1.,   22.]])"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_pxgxi(d_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LatentDirichletAllocation(batch_size=100, doc_topic_prior=None,\n",
       "             evaluate_every=-1, learning_decay=0.7,\n",
       "             learning_method='batch', learning_offset=10.0,\n",
       "             max_doc_update_iter=100, max_iter=10, mean_change_tol=0.001,\n",
       "             n_components=30, n_jobs=1, n_topics=None, perp_tol=0.1,\n",
       "             random_state=None, topic_word_prior=None,\n",
       "             total_samples=1000000.0, verbose=0)"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda = LatentDirichletAllocation(n_components=30, learning_method=\"batch\", batch_size=100)\n",
    "lda.fit(d_train.X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "ttrain = lda.transform(d_train.X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.enable_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis.sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.display(pyLDAvis.sklearn.prepare(lda, d_train.X, twitter.vectorizer, mds='mmds'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/virgile/.conda/envs/py36/lib/python3.6/site-packages/seaborn/categorical.py:1460: FutureWarning: remove_na is deprecated and is a private function. Do not use.\n",
      "  stat_data = remove_na(group_data)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f8160028da0>"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f8160028828>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.countplot(ttrain.argmax(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "ttest = lda.transform(d_test.X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12863.88940417501"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.perplexity(d_train.X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13496.632913821431"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.perplexity(d_test.X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shifting topics discovery"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run experiment that blocks one topic and return the prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_predictions_per_topic(model, weights_idx, X_test):\n",
    "    weights_og = model.get_weights()\n",
    "    \n",
    "    ntopics = weights_og[weights_idx].shape[0]\n",
    "    preds = []\n",
    "    for ti in tqdm(list(range(ntopics))):\n",
    "        weights = deepcopy(weights_og)\n",
    "        weights[-2][ti] = 0\n",
    "        model.set_weights(weights)\n",
    "        preds.append(model.predict(X_test))\n",
    "    \n",
    "    model.set_weights(weights_og)\n",
    "    \n",
    "    preds = np.hstack(preds)\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 5/50 [00:03<00:29,  1.52it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-4950fc887eab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# preds_per_topic = make_predictions_per_topic(vae_topicmodel.topic_model, 2, yelp.X[idx_bias09])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpreds_per_topic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_predictions_per_topic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdnn_clf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpred_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myelp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx_bias09\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-11-1b190fb86f84>\u001b[0m in \u001b[0;36mmake_predictions_per_topic\u001b[0;34m(model, weights_idx, X_test)\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mweights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mti\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mpreds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights_og\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/py36/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1170\u001b[0m                                             \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1171\u001b[0m                                             \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1172\u001b[0;31m                                             steps=steps)\n\u001b[0m\u001b[1;32m   1173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1174\u001b[0m     def train_on_batch(self, x, y,\n",
      "\u001b[0;32m~/.conda/envs/py36/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mpredict_loop\u001b[0;34m(model, f, ins, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m    295\u001b[0m                 \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 297\u001b[0;31m             \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    298\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m                 \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/py36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2665\u001b[0m                     \u001b[0;34m'In order to feed symbolic tensors to a Keras model '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2666\u001b[0m                     'in TensorFlow, you need tensorflow 1.8 or higher.')\n\u001b[0;32m-> 2667\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2668\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2669\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/py36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_legacy_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2647\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2648\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2649\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2650\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2651\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/py36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/py36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1120\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1121\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/py36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1317\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1318\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/py36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/py36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1300\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1302\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# preds_per_topic = make_predictions_per_topic(vae_topicmodel.topic_model, 2, yelp.X[idx_bias09])\n",
    "preds_per_topic = make_predictions_per_topic(dnn_clf.pred_model, 2, yelp.X[idx_bias09])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_per_topic_int = np.around(preds_per_topic).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode_per_instance = mode(preds_per_topic_int, axis=1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "changes_per_topic = (preds_per_topic_int != mode_per_instance).sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_weights = dnn_clf.pred_model.get_weights()[2].flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1. Topic</th>\n",
       "      <th>2. Weight</th>\n",
       "      <th>3. # Changes</th>\n",
       "      <th>4. Top Words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>34</td>\n",
       "      <td>-13.493729</td>\n",
       "      <td>2606</td>\n",
       "      <td>mediocre, rude, worst, horrible, terrible, bland, disappointing, poor, okay, ok, tasted, dirty, slow, sorry, shame</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>44</td>\n",
       "      <td>-2.783704</td>\n",
       "      <td>0</td>\n",
       "      <td>northwest, merlot, iteration, distraction, sever, german, macadamia, indians, drizzling, cambria, seam, sprays, hazelnuts, bah, bra</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>23</td>\n",
       "      <td>-2.636929</td>\n",
       "      <td>0</td>\n",
       "      <td>adobo, connie, dairy, crema, occurs, carafe, beds, baltimore, antney, ante, dishes, lemony, fusion, distilled, 6ish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>21</td>\n",
       "      <td>-1.839704</td>\n",
       "      <td>0</td>\n",
       "      <td>noun, losers, biscotti, carve, disguised, deposit, tavern, keeps, hibachi, trophy, lindsay, umami, upstairs, bombers, friends</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>39</td>\n",
       "      <td>-1.816739</td>\n",
       "      <td>0</td>\n",
       "      <td>approach, wristband, hazy, sal, täkō, socialize, profound, remedy, butcher, exceedingly, hitch, mont, babaganoush, technical, rack</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4</td>\n",
       "      <td>-1.465101</td>\n",
       "      <td>0</td>\n",
       "      <td>camera, swell, igloo, fluent, tataki, motioned, stews, alejandro, male, customization, moly, wonton, sung, morons, wowed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>38</td>\n",
       "      <td>-1.240233</td>\n",
       "      <td>0</td>\n",
       "      <td>bake, introduction, satisfied, preventing, bark, conscious, hyperbole, orgasm, cho, ragout, stairs, liquors, crummy, slider, ali</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>43</td>\n",
       "      <td>-1.170576</td>\n",
       "      <td>0</td>\n",
       "      <td>jerked, unlucky, moly, minimize, jules, malty, pterodactyl, kart, fundido, wanted, rice, unsuccessfully, cheerwine, unorganized, cavities</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>7</td>\n",
       "      <td>-1.123191</td>\n",
       "      <td>0</td>\n",
       "      <td>saks, lite, hay, desperately, dirtiest, snag, tamago, tamara, tractor, vacation, roethlisburger, mandy, venti, vinegary, plumber</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>-1.063330</td>\n",
       "      <td>0</td>\n",
       "      <td>repetitive, bird, empanada, fast, soundproofing, tuscany, swallowed, vacate, aw, legally, inca, dank, alarmed, hometown, malty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>35</td>\n",
       "      <td>-0.987791</td>\n",
       "      <td>0</td>\n",
       "      <td>pumpkin, schedule, tabasco, national, choo, lawyers, soleil, hells, huh, butternut, homeless, uneaten, simpleton, tins, interrupting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>16</td>\n",
       "      <td>-0.898605</td>\n",
       "      <td>0</td>\n",
       "      <td>joanna, emphatically, kyoto, soo, https, chef, basic, explosive, lines, slammin, immersed, diarrhea, classically, restaurants, removal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>26</td>\n",
       "      <td>-0.780818</td>\n",
       "      <td>0</td>\n",
       "      <td>mexicali, fresno, venezuelan, gaucho, accomodating, holidays, pamela, selections, mmmmmmmmmmm, chuys, brewmaster, hack, fantabulous, trick, dammit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>41</td>\n",
       "      <td>-0.762073</td>\n",
       "      <td>0</td>\n",
       "      <td>drapes, teow, terror, falafels, frankly, lust, adobo, versatile, climax, avail, yikes, gastro, teapot, rut, abd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>33</td>\n",
       "      <td>-0.707245</td>\n",
       "      <td>0</td>\n",
       "      <td>drips, dies, rican, unicorn, brie, eclectic, jerusalem, midtown, tinga, scam, configured, wilkinsburg, acacia, shepard, write</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>46</td>\n",
       "      <td>-0.703957</td>\n",
       "      <td>0</td>\n",
       "      <td>huevos, papi, mia, agency, packet, lululemon, spilt, friendlier, heavy, wingstop, farro, accountant, likable, mount, saccharine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>42</td>\n",
       "      <td>-0.688086</td>\n",
       "      <td>0</td>\n",
       "      <td>blending, kamikaze, jenga, carter, succession, chowed, profession, needy, patatas, planing, merchandise, feast, international, tori, popeyes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>36</td>\n",
       "      <td>-0.667798</td>\n",
       "      <td>0</td>\n",
       "      <td>brilliantly, hella, body, sherbet, period, achievement, cheery, foresee, brioche, ne, bonito, interior, gaze, classical, circled</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>11</td>\n",
       "      <td>-0.632631</td>\n",
       "      <td>0</td>\n",
       "      <td>ihg, fluctuate, mixologists, attends, alpha, 100, mexicali, compromising, fir, wont, report, grandmother, earn, unacceptable, fused</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>25</td>\n",
       "      <td>-0.565828</td>\n",
       "      <td>0</td>\n",
       "      <td>mistaking, appitizer, vodka, cyros, kahlua, 485, pio, diagonal, wahoo, excite, fuck, technicians, churrascaria, kickstand, pizzaiolo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>13</td>\n",
       "      <td>-0.070104</td>\n",
       "      <td>0</td>\n",
       "      <td>buck, stripped, mow, signatures, buffett, jamming, megan, peppermint, abundance, selves, explanatory, digit, travelling, fascination, firearm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>6</td>\n",
       "      <td>-0.065300</td>\n",
       "      <td>0</td>\n",
       "      <td>pun, alerted, reception, inconveniencing, café, lids, 1500, b4, soothe, expansion, beast, unfortunate, mano, joanna, mundane</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>20</td>\n",
       "      <td>-0.060950</td>\n",
       "      <td>0</td>\n",
       "      <td>distractions, panels, teas, oversold, tessaro, refreshing, bodies, miscellaneous, planted, fox, watered, wet, design, wired, teachers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1</td>\n",
       "      <td>-0.035327</td>\n",
       "      <td>0</td>\n",
       "      <td>saltado, pose, tape, sirius, tuck, remoulade, tamale, uploaded, avacado, number, expire, concept, meet, openings, molten</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>29</td>\n",
       "      <td>-0.034952</td>\n",
       "      <td>0</td>\n",
       "      <td>men, indians, funeral, daring, invite, charleston, hated, robo, bottoms, aplenty, jalapeno, carribean, channels, immature, 2000s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>28</td>\n",
       "      <td>-0.020784</td>\n",
       "      <td>0</td>\n",
       "      <td>mario, biting, rod, healthiest, gene, lotion, fantastically, harshly, grist, waistline, mixologist, labels, ordinarily, stayed, disrespected</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>49</td>\n",
       "      <td>0.257327</td>\n",
       "      <td>64</td>\n",
       "      <td>dogwood, sogginess, ordered, hommos, tantalizing, tartness, attacking, sneak, management, conveyor, soup, alot, tried, tartine, tasters</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>48</td>\n",
       "      <td>0.280567</td>\n",
       "      <td>71</td>\n",
       "      <td>cold, complimenting, zalad, soleil, vocals, blueberries, hold, dissapointing, gorgeously, excellent, bring, shin, banking, preserved, worker</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>3</td>\n",
       "      <td>0.298930</td>\n",
       "      <td>84</td>\n",
       "      <td>garlic, right, joe, relieve, insisting, conscience, trend, barbecue, server, shui, choppy, tempeh, planner, worldwide, dancer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0</td>\n",
       "      <td>0.313282</td>\n",
       "      <td>89</td>\n",
       "      <td>piling, drinks, dope, greek, stuff, public, oily, harra, waxer, i2pa, burrata, zacks, mentioned, goat, harissa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>45</td>\n",
       "      <td>0.321882</td>\n",
       "      <td>88</td>\n",
       "      <td>caipirinha, caveats, brine, tjs, slightly, sat, proud, lightening, lager, okra, pickles, hors, byrd, woulda, passionately</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>22</td>\n",
       "      <td>0.323753</td>\n",
       "      <td>88</td>\n",
       "      <td>bucks, box, house, chuys, salim, delightful, sat, gentrified, pineville, cream, stems, mussaman, korea, greek, utensil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>24</td>\n",
       "      <td>0.340351</td>\n",
       "      <td>98</td>\n",
       "      <td>cheap, stuff, congested, wags, cevapi, selection, place, sorry, descriptive, gem, shorty, server, dishware, reserving, tong</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>2</td>\n",
       "      <td>0.343092</td>\n",
       "      <td>95</td>\n",
       "      <td>playful, looked, amigos, tatted, ihg, terrine, new, product, expect, kendra, zacks, method, dearly, cheap, amazing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>18</td>\n",
       "      <td>0.396208</td>\n",
       "      <td>128</td>\n",
       "      <td>right, come, place, cheap, ravioli, cold, ordered, david, old, trader, helpful, freshener, shroom, mainly, night</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>47</td>\n",
       "      <td>0.400963</td>\n",
       "      <td>133</td>\n",
       "      <td>ordered, bobby, cold, cheap, greek, garlic, merchandise, drinks, goat, prayers, just, pointe, complications, salmon, butterscotch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>30</td>\n",
       "      <td>0.483964</td>\n",
       "      <td>161</td>\n",
       "      <td>bobby, old, slightly, place, just, bit, stars, outnumbered, gotten, incorporates, beers, salads, right, game, perfect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>27</td>\n",
       "      <td>0.493852</td>\n",
       "      <td>163</td>\n",
       "      <td>bobby, plate, ordered, right, cheap, french, new, lunasagna, bookshelf, sat, pho, vip, multi, drinks, unfinished</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>40</td>\n",
       "      <td>0.553698</td>\n",
       "      <td>188</td>\n",
       "      <td>bobby, pizza, just, amazing, 25, liken, burlington, savoring, couple, sandwiches, delightful, french, prepared, eat, wireless</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>37</td>\n",
       "      <td>0.598970</td>\n",
       "      <td>213</td>\n",
       "      <td>ordered, bobby, just, amazing, really, peppers, ravioli, plate, creamy, sorrento, combination, aged, braved, alex, decent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>19</td>\n",
       "      <td>0.611859</td>\n",
       "      <td>212</td>\n",
       "      <td>cold, just, bobby, pizza, ordered, cheap, wine, place, use, 30, layovers, flavorful, typical, indy, beat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>12</td>\n",
       "      <td>0.652265</td>\n",
       "      <td>236</td>\n",
       "      <td>ordered, sat, pizza, just, amazing, bobby, cold, old, pretty, ravioli, cooked, right, new, wine, garlic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>32</td>\n",
       "      <td>0.828805</td>\n",
       "      <td>320</td>\n",
       "      <td>bobby, pizza, ordered, just, cold, service, sugar, amazing, greek, excellent, old, target, sat, people, french</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>14</td>\n",
       "      <td>0.872585</td>\n",
       "      <td>349</td>\n",
       "      <td>bobby, pizza, people, sat, service, just, amazing, old, ordered, sugar, tomato, target, like, sure, cold</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>15</td>\n",
       "      <td>0.971088</td>\n",
       "      <td>383</td>\n",
       "      <td>people, bobby, pizza, sat, just, greek, ordered, service, french, cold, stars, old, coming, target, like</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>17</td>\n",
       "      <td>1.074475</td>\n",
       "      <td>439</td>\n",
       "      <td>people, bobby, pizza, greek, cold, sat, just, mart, wal, service, target, coming, amazing, ordered, like</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>8</td>\n",
       "      <td>1.256491</td>\n",
       "      <td>519</td>\n",
       "      <td>bobby, people, greek, sat, wal, target, pizza, delicious, cold, just, amazing, excellent, ordered, mart, long</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>5</td>\n",
       "      <td>1.521764</td>\n",
       "      <td>653</td>\n",
       "      <td>greek, bobby, people, excellent, sat, target, wal, delicious, pizza, cold, amazing, ordered, mart, french, just</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>31</td>\n",
       "      <td>1.935102</td>\n",
       "      <td>835</td>\n",
       "      <td>excellent, greek, bobby, target, people, delicious, sat, mart, wal, long, cold, old, pizza, right, amazing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>10</td>\n",
       "      <td>9.172626</td>\n",
       "      <td>1118</td>\n",
       "      <td>class, nice, great, skin, treatments, continue, guys, skills, mole, room, better, women, review, brazilian, think</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    1. Topic  2. Weight  3. # Changes  \\\n",
       "0         34 -13.493729          2606   \n",
       "1         44  -2.783704             0   \n",
       "2         23  -2.636929             0   \n",
       "3         21  -1.839704             0   \n",
       "4         39  -1.816739             0   \n",
       "5          4  -1.465101             0   \n",
       "6         38  -1.240233             0   \n",
       "7         43  -1.170576             0   \n",
       "8          7  -1.123191             0   \n",
       "9          9  -1.063330             0   \n",
       "10        35  -0.987791             0   \n",
       "11        16  -0.898605             0   \n",
       "12        26  -0.780818             0   \n",
       "13        41  -0.762073             0   \n",
       "14        33  -0.707245             0   \n",
       "15        46  -0.703957             0   \n",
       "16        42  -0.688086             0   \n",
       "17        36  -0.667798             0   \n",
       "18        11  -0.632631             0   \n",
       "19        25  -0.565828             0   \n",
       "20        13  -0.070104             0   \n",
       "21         6  -0.065300             0   \n",
       "22        20  -0.060950             0   \n",
       "23         1  -0.035327             0   \n",
       "24        29  -0.034952             0   \n",
       "25        28  -0.020784             0   \n",
       "26        49   0.257327            64   \n",
       "27        48   0.280567            71   \n",
       "28         3   0.298930            84   \n",
       "29         0   0.313282            89   \n",
       "30        45   0.321882            88   \n",
       "31        22   0.323753            88   \n",
       "32        24   0.340351            98   \n",
       "33         2   0.343092            95   \n",
       "34        18   0.396208           128   \n",
       "35        47   0.400963           133   \n",
       "36        30   0.483964           161   \n",
       "37        27   0.493852           163   \n",
       "38        40   0.553698           188   \n",
       "39        37   0.598970           213   \n",
       "40        19   0.611859           212   \n",
       "41        12   0.652265           236   \n",
       "42        32   0.828805           320   \n",
       "43        14   0.872585           349   \n",
       "44        15   0.971088           383   \n",
       "45        17   1.074475           439   \n",
       "46         8   1.256491           519   \n",
       "47         5   1.521764           653   \n",
       "48        31   1.935102           835   \n",
       "49        10   9.172626          1118   \n",
       "\n",
       "                                                                                                                                          4. Top Words  \n",
       "0                                   mediocre, rude, worst, horrible, terrible, bland, disappointing, poor, okay, ok, tasted, dirty, slow, sorry, shame  \n",
       "1                  northwest, merlot, iteration, distraction, sever, german, macadamia, indians, drizzling, cambria, seam, sprays, hazelnuts, bah, bra  \n",
       "2                                  adobo, connie, dairy, crema, occurs, carafe, beds, baltimore, antney, ante, dishes, lemony, fusion, distilled, 6ish  \n",
       "3                        noun, losers, biscotti, carve, disguised, deposit, tavern, keeps, hibachi, trophy, lindsay, umami, upstairs, bombers, friends  \n",
       "4                   approach, wristband, hazy, sal, täkō, socialize, profound, remedy, butcher, exceedingly, hitch, mont, babaganoush, technical, rack  \n",
       "5                             camera, swell, igloo, fluent, tataki, motioned, stews, alejandro, male, customization, moly, wonton, sung, morons, wowed  \n",
       "6                     bake, introduction, satisfied, preventing, bark, conscious, hyperbole, orgasm, cho, ragout, stairs, liquors, crummy, slider, ali  \n",
       "7            jerked, unlucky, moly, minimize, jules, malty, pterodactyl, kart, fundido, wanted, rice, unsuccessfully, cheerwine, unorganized, cavities  \n",
       "8                     saks, lite, hay, desperately, dirtiest, snag, tamago, tamara, tractor, vacation, roethlisburger, mandy, venti, vinegary, plumber  \n",
       "9                       repetitive, bird, empanada, fast, soundproofing, tuscany, swallowed, vacate, aw, legally, inca, dank, alarmed, hometown, malty  \n",
       "10                pumpkin, schedule, tabasco, national, choo, lawyers, soleil, hells, huh, butternut, homeless, uneaten, simpleton, tins, interrupting  \n",
       "11              joanna, emphatically, kyoto, soo, https, chef, basic, explosive, lines, slammin, immersed, diarrhea, classically, restaurants, removal  \n",
       "12  mexicali, fresno, venezuelan, gaucho, accomodating, holidays, pamela, selections, mmmmmmmmmmm, chuys, brewmaster, hack, fantabulous, trick, dammit  \n",
       "13                                     drapes, teow, terror, falafels, frankly, lust, adobo, versatile, climax, avail, yikes, gastro, teapot, rut, abd  \n",
       "14                       drips, dies, rican, unicorn, brie, eclectic, jerusalem, midtown, tinga, scam, configured, wilkinsburg, acacia, shepard, write  \n",
       "15                     huevos, papi, mia, agency, packet, lululemon, spilt, friendlier, heavy, wingstop, farro, accountant, likable, mount, saccharine  \n",
       "16        blending, kamikaze, jenga, carter, succession, chowed, profession, needy, patatas, planing, merchandise, feast, international, tori, popeyes  \n",
       "17                    brilliantly, hella, body, sherbet, period, achievement, cheery, foresee, brioche, ne, bonito, interior, gaze, classical, circled  \n",
       "18                 ihg, fluctuate, mixologists, attends, alpha, 100, mexicali, compromising, fir, wont, report, grandmother, earn, unacceptable, fused  \n",
       "19                mistaking, appitizer, vodka, cyros, kahlua, 485, pio, diagonal, wahoo, excite, fuck, technicians, churrascaria, kickstand, pizzaiolo  \n",
       "20       buck, stripped, mow, signatures, buffett, jamming, megan, peppermint, abundance, selves, explanatory, digit, travelling, fascination, firearm  \n",
       "21                        pun, alerted, reception, inconveniencing, café, lids, 1500, b4, soothe, expansion, beast, unfortunate, mano, joanna, mundane  \n",
       "22               distractions, panels, teas, oversold, tessaro, refreshing, bodies, miscellaneous, planted, fox, watered, wet, design, wired, teachers  \n",
       "23                            saltado, pose, tape, sirius, tuck, remoulade, tamale, uploaded, avacado, number, expire, concept, meet, openings, molten  \n",
       "24                    men, indians, funeral, daring, invite, charleston, hated, robo, bottoms, aplenty, jalapeno, carribean, channels, immature, 2000s  \n",
       "25        mario, biting, rod, healthiest, gene, lotion, fantastically, harshly, grist, waistline, mixologist, labels, ordinarily, stayed, disrespected  \n",
       "26             dogwood, sogginess, ordered, hommos, tantalizing, tartness, attacking, sneak, management, conveyor, soup, alot, tried, tartine, tasters  \n",
       "27        cold, complimenting, zalad, soleil, vocals, blueberries, hold, dissapointing, gorgeously, excellent, bring, shin, banking, preserved, worker  \n",
       "28                       garlic, right, joe, relieve, insisting, conscience, trend, barbecue, server, shui, choppy, tempeh, planner, worldwide, dancer  \n",
       "29                                      piling, drinks, dope, greek, stuff, public, oily, harra, waxer, i2pa, burrata, zacks, mentioned, goat, harissa  \n",
       "30                           caipirinha, caveats, brine, tjs, slightly, sat, proud, lightening, lager, okra, pickles, hors, byrd, woulda, passionately  \n",
       "31                              bucks, box, house, chuys, salim, delightful, sat, gentrified, pineville, cream, stems, mussaman, korea, greek, utensil  \n",
       "32                         cheap, stuff, congested, wags, cevapi, selection, place, sorry, descriptive, gem, shorty, server, dishware, reserving, tong  \n",
       "33                                  playful, looked, amigos, tatted, ihg, terrine, new, product, expect, kendra, zacks, method, dearly, cheap, amazing  \n",
       "34                                    right, come, place, cheap, ravioli, cold, ordered, david, old, trader, helpful, freshener, shroom, mainly, night  \n",
       "35                   ordered, bobby, cold, cheap, greek, garlic, merchandise, drinks, goat, prayers, just, pointe, complications, salmon, butterscotch  \n",
       "36                               bobby, old, slightly, place, just, bit, stars, outnumbered, gotten, incorporates, beers, salads, right, game, perfect  \n",
       "37                                    bobby, plate, ordered, right, cheap, french, new, lunasagna, bookshelf, sat, pho, vip, multi, drinks, unfinished  \n",
       "38                       bobby, pizza, just, amazing, 25, liken, burlington, savoring, couple, sandwiches, delightful, french, prepared, eat, wireless  \n",
       "39                           ordered, bobby, just, amazing, really, peppers, ravioli, plate, creamy, sorrento, combination, aged, braved, alex, decent  \n",
       "40                                            cold, just, bobby, pizza, ordered, cheap, wine, place, use, 30, layovers, flavorful, typical, indy, beat  \n",
       "41                                             ordered, sat, pizza, just, amazing, bobby, cold, old, pretty, ravioli, cooked, right, new, wine, garlic  \n",
       "42                                      bobby, pizza, ordered, just, cold, service, sugar, amazing, greek, excellent, old, target, sat, people, french  \n",
       "43                                            bobby, pizza, people, sat, service, just, amazing, old, ordered, sugar, tomato, target, like, sure, cold  \n",
       "44                                            people, bobby, pizza, sat, just, greek, ordered, service, french, cold, stars, old, coming, target, like  \n",
       "45                                            people, bobby, pizza, greek, cold, sat, just, mart, wal, service, target, coming, amazing, ordered, like  \n",
       "46                                       bobby, people, greek, sat, wal, target, pizza, delicious, cold, just, amazing, excellent, ordered, mart, long  \n",
       "47                                     greek, bobby, people, excellent, sat, target, wal, delicious, pizza, cold, amazing, ordered, mart, french, just  \n",
       "48                                          excellent, greek, bobby, target, people, delicious, sat, mart, wal, long, cold, old, pizza, right, amazing  \n",
       "49                                   class, nice, great, skin, treatments, continue, guys, skills, mole, room, better, women, review, brazilian, think  "
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = dnn_clf.pred_model.get_weights()\n",
    "pd.DataFrame([{\n",
    "    '1. Topic': idx,\n",
    "    '2. Weight': topic_weights[idx],\n",
    "    '3. # Changes': changes_per_topic[idx],\n",
    "    '4. Top Words': \", \".join(yelp.feature_names[weights[0][:,idx].argsort()[::-1]][:30])\n",
    "} for idx in topic_weights.argsort()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VAE model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'adaptive_confound.topic_model' from '/home/virgile/adaptive_confound/adaptive_confound/topic_model/__init__.py'>"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reload(actm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae = actm.VariationalAutoEncoder(\n",
    "    k=twitter.features.size,\n",
    "    m=30, \n",
    "    model_params=dict(\n",
    "        hidden_dim=100,\n",
    "        beta=1,\n",
    "        supervised=False\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = vae.topic_model.weights[0]\n",
    "b = vae.topic_model.weights[2]\n",
    "tw = sess.run(tf.tensordot(a, b, [[1],[0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21981, 30)"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tw = sess.run(vae_topicmodel.topic_model.weights[0])\n",
    "tw.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2025"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(twitter.features == 'basketball')[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025 basketball\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Container object of 30 artists>"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fd8ee1d30b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fti = np.where(twitter.features == 'basketball')[0][0] #np.random.choice(range(yelp.feature_names.size))\n",
    "print(fti, twitter.features[fti])\n",
    "plt.bar(range(tw.shape[1]), tw[fti])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>needs [3.91e-01]</td>\n",
       "      <td>needs [3.75e-01]</td>\n",
       "      <td>sunset [1.74e-01]</td>\n",
       "      <td>needs [3.29e-01]</td>\n",
       "      <td>needs [4.08e-01]</td>\n",
       "      <td>ranch [2.62e-01]</td>\n",
       "      <td>needs [2.65e-01]</td>\n",
       "      <td>sunset [2.15e-01]</td>\n",
       "      <td>needs [3.84e-01]</td>\n",
       "      <td>argentina [8.58e-02]</td>\n",
       "      <td>romance [6.62e-02]</td>\n",
       "      <td>sneezing [5.39e-02]</td>\n",
       "      <td>ahhhhh [1.36e-01]</td>\n",
       "      <td>needs [2.09e-01]</td>\n",
       "      <td>heavy [1.31e-01]</td>\n",
       "      <td>needs [2.40e-01]</td>\n",
       "      <td>needs [2.42e-01]</td>\n",
       "      <td>sourcing [5.22e-02]</td>\n",
       "      <td>hair [8.45e-02]</td>\n",
       "      <td>smallest [7.48e-02]</td>\n",
       "      <td>jerk [7.31e-02]</td>\n",
       "      <td>2010 [1.43e-01]</td>\n",
       "      <td>cyclists [6.14e-02]</td>\n",
       "      <td>forward [9.12e-02]</td>\n",
       "      <td>needs [2.56e-01]</td>\n",
       "      <td>lack [1.59e-01]</td>\n",
       "      <td>dias [9.00e-02]</td>\n",
       "      <td>needs [2.87e-01]</td>\n",
       "      <td>ranch [2.77e-01]</td>\n",
       "      <td>jurassic [5.62e-02]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ranch [3.50e-01]</td>\n",
       "      <td>mother [3.39e-01]</td>\n",
       "      <td>needs [1.72e-01]</td>\n",
       "      <td>sunset [3.18e-01]</td>\n",
       "      <td>ranch [4.05e-01]</td>\n",
       "      <td>mother [2.60e-01]</td>\n",
       "      <td>ranch [2.65e-01]</td>\n",
       "      <td>ranch [2.10e-01]</td>\n",
       "      <td>ranch [3.68e-01]</td>\n",
       "      <td>hmmmmm [8.36e-02]</td>\n",
       "      <td>cleaning [6.57e-02]</td>\n",
       "      <td>resisting [5.08e-02]</td>\n",
       "      <td>appreciated [1.31e-01]</td>\n",
       "      <td>ranch [2.07e-01]</td>\n",
       "      <td>outta [1.30e-01]</td>\n",
       "      <td>ranch [2.23e-01]</td>\n",
       "      <td>sunset [2.21e-01]</td>\n",
       "      <td>fcc [5.06e-02]</td>\n",
       "      <td>research [8.12e-02]</td>\n",
       "      <td>left [7.29e-02]</td>\n",
       "      <td>favorites [6.46e-02]</td>\n",
       "      <td>killed [1.41e-01]</td>\n",
       "      <td>threads [5.74e-02]</td>\n",
       "      <td>shows [9.12e-02]</td>\n",
       "      <td>ranch [2.49e-01]</td>\n",
       "      <td>contract [1.55e-01]</td>\n",
       "      <td>artist [8.95e-02]</td>\n",
       "      <td>mother [2.86e-01]</td>\n",
       "      <td>needs [2.39e-01]</td>\n",
       "      <td>rec [5.62e-02]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sunset [3.45e-01]</td>\n",
       "      <td>sunset [3.39e-01]</td>\n",
       "      <td>mother [1.70e-01]</td>\n",
       "      <td>ranch [3.14e-01]</td>\n",
       "      <td>mother [3.98e-01]</td>\n",
       "      <td>needs [2.55e-01]</td>\n",
       "      <td>mother [2.62e-01]</td>\n",
       "      <td>needs [2.09e-01]</td>\n",
       "      <td>mother [3.62e-01]</td>\n",
       "      <td>removed [7.51e-02]</td>\n",
       "      <td>nut [6.44e-02]</td>\n",
       "      <td>cielo [4.80e-02]</td>\n",
       "      <td>patience [1.28e-01]</td>\n",
       "      <td>mother [2.02e-01]</td>\n",
       "      <td>billboard [1.21e-01]</td>\n",
       "      <td>mother [2.08e-01]</td>\n",
       "      <td>ranch [2.20e-01]</td>\n",
       "      <td>amar [4.90e-02]</td>\n",
       "      <td>facebook [7.75e-02]</td>\n",
       "      <td>shazam [7.26e-02]</td>\n",
       "      <td>ahhhhh [6.30e-02]</td>\n",
       "      <td>horrible [1.41e-01]</td>\n",
       "      <td>slideshare [5.40e-02]</td>\n",
       "      <td>post [9.01e-02]</td>\n",
       "      <td>mother [2.44e-01]</td>\n",
       "      <td>2010 [1.52e-01]</td>\n",
       "      <td>kit [8.66e-02]</td>\n",
       "      <td>ranch [2.70e-01]</td>\n",
       "      <td>mother [2.34e-01]</td>\n",
       "      <td>bird [5.06e-02]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mother [3.44e-01]</td>\n",
       "      <td>ranch [3.27e-01]</td>\n",
       "      <td>ranch [1.64e-01]</td>\n",
       "      <td>mother [3.07e-01]</td>\n",
       "      <td>sunset [3.78e-01]</td>\n",
       "      <td>sunset [2.30e-01]</td>\n",
       "      <td>sunset [2.49e-01]</td>\n",
       "      <td>mother [1.98e-01]</td>\n",
       "      <td>sunset [3.22e-01]</td>\n",
       "      <td>praying [7.18e-02]</td>\n",
       "      <td>gosling [6.38e-02]</td>\n",
       "      <td>kik [4.72e-02]</td>\n",
       "      <td>sundays [1.27e-01]</td>\n",
       "      <td>kiss [1.92e-01]</td>\n",
       "      <td>kit [1.17e-01]</td>\n",
       "      <td>sunset [1.98e-01]</td>\n",
       "      <td>mother [2.10e-01]</td>\n",
       "      <td>cafe [4.81e-02]</td>\n",
       "      <td>cafe [7.75e-02]</td>\n",
       "      <td>dipped [7.03e-02]</td>\n",
       "      <td>grandpa [5.95e-02]</td>\n",
       "      <td>unless [1.40e-01]</td>\n",
       "      <td>sochi [5.33e-02]</td>\n",
       "      <td>lt [8.91e-02]</td>\n",
       "      <td>sunset [2.19e-01]</td>\n",
       "      <td>thankfully [1.51e-01]</td>\n",
       "      <td>frozen [8.61e-02]</td>\n",
       "      <td>kiss [2.69e-01]</td>\n",
       "      <td>sunset [2.33e-01]</td>\n",
       "      <td>highschoolsuckz [5.02e-02]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>kiss [3.13e-01]</td>\n",
       "      <td>kiss [3.14e-01]</td>\n",
       "      <td>kiss [1.52e-01]</td>\n",
       "      <td>kiss [3.00e-01]</td>\n",
       "      <td>kiss [3.60e-01]</td>\n",
       "      <td>kiss [2.00e-01]</td>\n",
       "      <td>kiss [2.39e-01]</td>\n",
       "      <td>kiss [1.71e-01]</td>\n",
       "      <td>kiss [3.16e-01]</td>\n",
       "      <td>dre [7.17e-02]</td>\n",
       "      <td>nutella [6.36e-02]</td>\n",
       "      <td>indictment [4.62e-02]</td>\n",
       "      <td>billboard [1.26e-01]</td>\n",
       "      <td>sunset [1.83e-01]</td>\n",
       "      <td>patience [1.14e-01]</td>\n",
       "      <td>kiss [1.76e-01]</td>\n",
       "      <td>kiss [2.08e-01]</td>\n",
       "      <td>nears [4.72e-02]</td>\n",
       "      <td>loves [7.72e-02]</td>\n",
       "      <td>da [7.02e-02]</td>\n",
       "      <td>hasn [5.88e-02]</td>\n",
       "      <td>roll [1.39e-01]</td>\n",
       "      <td>kahn [5.20e-02]</td>\n",
       "      <td>holler [8.76e-02]</td>\n",
       "      <td>kiss [2.17e-01]</td>\n",
       "      <td>cooler [1.50e-01]</td>\n",
       "      <td>grand [8.47e-02]</td>\n",
       "      <td>sunset [2.57e-01]</td>\n",
       "      <td>kiss [2.11e-01]</td>\n",
       "      <td>conspiracyimage [4.80e-02]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>yesterday [2.15e-01]</td>\n",
       "      <td>hershey [1.84e-01]</td>\n",
       "      <td>empowering [7.15e-02]</td>\n",
       "      <td>empowering [1.86e-01]</td>\n",
       "      <td>awesome [2.21e-01]</td>\n",
       "      <td>picture [1.19e-01]</td>\n",
       "      <td>picture [9.85e-02]</td>\n",
       "      <td>picture [8.51e-02]</td>\n",
       "      <td>picture [1.76e-01]</td>\n",
       "      <td>zeppelin [7.10e-02]</td>\n",
       "      <td>bbq [6.35e-02]</td>\n",
       "      <td>cedar [4.61e-02]</td>\n",
       "      <td>cooler [1.25e-01]</td>\n",
       "      <td>educate [1.09e-01]</td>\n",
       "      <td>hood [1.13e-01]</td>\n",
       "      <td>yesterday [9.90e-02]</td>\n",
       "      <td>educate [1.02e-01]</td>\n",
       "      <td>pelican [4.72e-02]</td>\n",
       "      <td>foursquare [7.67e-02]</td>\n",
       "      <td>bali [6.99e-02]</td>\n",
       "      <td>gigs [5.85e-02]</td>\n",
       "      <td>iggy [1.35e-01]</td>\n",
       "      <td>compact [5.01e-02]</td>\n",
       "      <td>era [8.64e-02]</td>\n",
       "      <td>picture [8.20e-02]</td>\n",
       "      <td>industry [1.49e-01]</td>\n",
       "      <td>haha [8.46e-02]</td>\n",
       "      <td>hershey [1.60e-01]</td>\n",
       "      <td>implications [1.18e-01]</td>\n",
       "      <td>shawshank [4.78e-02]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>working [1.88e-01]</td>\n",
       "      <td>picture [1.71e-01]</td>\n",
       "      <td>sa [6.51e-02]</td>\n",
       "      <td>yesterday [1.76e-01]</td>\n",
       "      <td>picture [2.09e-01]</td>\n",
       "      <td>jurassic [1.18e-01]</td>\n",
       "      <td>manti [9.32e-02]</td>\n",
       "      <td>empowering [7.50e-02]</td>\n",
       "      <td>yesterday [1.51e-01]</td>\n",
       "      <td>lead [7.02e-02]</td>\n",
       "      <td>backwards [6.23e-02]</td>\n",
       "      <td>registering [4.56e-02]</td>\n",
       "      <td>sc [1.24e-01]</td>\n",
       "      <td>awesome [8.49e-02]</td>\n",
       "      <td>heads [1.13e-01]</td>\n",
       "      <td>hershey [9.83e-02]</td>\n",
       "      <td>picture [9.78e-02]</td>\n",
       "      <td>demonstrates [4.67e-02]</td>\n",
       "      <td>hopes [7.63e-02]</td>\n",
       "      <td>candidate [6.94e-02]</td>\n",
       "      <td>silicon [5.73e-02]</td>\n",
       "      <td>contract [1.33e-01]</td>\n",
       "      <td>nyc [4.99e-02]</td>\n",
       "      <td>sad [8.50e-02]</td>\n",
       "      <td>hershey [7.46e-02]</td>\n",
       "      <td>bat [1.48e-01]</td>\n",
       "      <td>lo [8.37e-02]</td>\n",
       "      <td>empowering [1.46e-01]</td>\n",
       "      <td>picture [1.17e-01]</td>\n",
       "      <td>upppp [4.75e-02]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>picture [1.81e-01]</td>\n",
       "      <td>yesterday [1.67e-01]</td>\n",
       "      <td>educate [6.51e-02]</td>\n",
       "      <td>awesome [1.71e-01]</td>\n",
       "      <td>yesterday [1.90e-01]</td>\n",
       "      <td>yesterday [1.14e-01]</td>\n",
       "      <td>educate [7.47e-02]</td>\n",
       "      <td>implications [7.06e-02]</td>\n",
       "      <td>hershey [1.49e-01]</td>\n",
       "      <td>dgaf [6.98e-02]</td>\n",
       "      <td>due [6.18e-02]</td>\n",
       "      <td>desperation [4.54e-02]</td>\n",
       "      <td>frustrated [1.24e-01]</td>\n",
       "      <td>hershey [7.67e-02]</td>\n",
       "      <td>helps [1.13e-01]</td>\n",
       "      <td>kisses [9.09e-02]</td>\n",
       "      <td>iphone [9.03e-02]</td>\n",
       "      <td>biden [4.64e-02]</td>\n",
       "      <td>lady [7.56e-02]</td>\n",
       "      <td>comparable [6.90e-02]</td>\n",
       "      <td>naughty [5.69e-02]</td>\n",
       "      <td>heads [1.33e-01]</td>\n",
       "      <td>engaging [4.90e-02]</td>\n",
       "      <td>star [8.22e-02]</td>\n",
       "      <td>educate [6.39e-02]</td>\n",
       "      <td>serious [1.44e-01]</td>\n",
       "      <td>total [8.35e-02]</td>\n",
       "      <td>implications [1.40e-01]</td>\n",
       "      <td>students [1.15e-01]</td>\n",
       "      <td>waaay [4.70e-02]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>hershey [1.79e-01]</td>\n",
       "      <td>awesome [1.60e-01]</td>\n",
       "      <td>picture [5.16e-02]</td>\n",
       "      <td>hershey [1.64e-01]</td>\n",
       "      <td>kisses [1.77e-01]</td>\n",
       "      <td>stores [1.12e-01]</td>\n",
       "      <td>stormtrooper [7.24e-02]</td>\n",
       "      <td>online [6.61e-02]</td>\n",
       "      <td>educate [1.39e-01]</td>\n",
       "      <td>bat [6.85e-02]</td>\n",
       "      <td>dry [6.06e-02]</td>\n",
       "      <td>werewolf [4.50e-02]</td>\n",
       "      <td>alert [1.24e-01]</td>\n",
       "      <td>jcolenc [7.32e-02]</td>\n",
       "      <td>intelligent [1.13e-01]</td>\n",
       "      <td>implications [8.22e-02]</td>\n",
       "      <td>awesome [8.95e-02]</td>\n",
       "      <td>dayton [4.64e-02]</td>\n",
       "      <td>americano [7.55e-02]</td>\n",
       "      <td>27 [6.88e-02]</td>\n",
       "      <td>thousands [5.66e-02]</td>\n",
       "      <td>helps [1.31e-01]</td>\n",
       "      <td>pena [4.87e-02]</td>\n",
       "      <td>topic [8.10e-02]</td>\n",
       "      <td>sa [6.33e-02]</td>\n",
       "      <td>9am [1.44e-01]</td>\n",
       "      <td>flower [8.18e-02]</td>\n",
       "      <td>picture [1.30e-01]</td>\n",
       "      <td>educate [1.02e-01]</td>\n",
       "      <td>iphone [4.69e-02]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>awesome [1.75e-01]</td>\n",
       "      <td>empowering [1.48e-01]</td>\n",
       "      <td>awesome [5.11e-02]</td>\n",
       "      <td>sa [1.60e-01]</td>\n",
       "      <td>implications [1.77e-01]</td>\n",
       "      <td>learning [1.09e-01]</td>\n",
       "      <td>yesterday [6.85e-02]</td>\n",
       "      <td>manti [6.58e-02]</td>\n",
       "      <td>awesome [1.39e-01]</td>\n",
       "      <td>ta [6.76e-02]</td>\n",
       "      <td>o_o [6.05e-02]</td>\n",
       "      <td>prisoners [4.48e-02]</td>\n",
       "      <td>root [1.23e-01]</td>\n",
       "      <td>manti [7.24e-02]</td>\n",
       "      <td>lee [1.12e-01]</td>\n",
       "      <td>sa [8.07e-02]</td>\n",
       "      <td>empowering [8.86e-02]</td>\n",
       "      <td>tap [4.64e-02]</td>\n",
       "      <td>landing [7.42e-02]</td>\n",
       "      <td>shirt [6.88e-02]</td>\n",
       "      <td>keeps [5.65e-02]</td>\n",
       "      <td>ended [1.31e-01]</td>\n",
       "      <td>vaguely [4.84e-02]</td>\n",
       "      <td>starwars [8.04e-02]</td>\n",
       "      <td>flames [6.29e-02]</td>\n",
       "      <td>complaining [1.44e-01]</td>\n",
       "      <td>appreciated [8.13e-02]</td>\n",
       "      <td>yesterday [1.30e-01]</td>\n",
       "      <td>learning [1.01e-01]</td>\n",
       "      <td>scarred [4.64e-02]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>educate [1.69e-01]</td>\n",
       "      <td>macys [1.42e-01]</td>\n",
       "      <td>longtime [4.98e-02]</td>\n",
       "      <td>picture [1.59e-01]</td>\n",
       "      <td>empowering [1.71e-01]</td>\n",
       "      <td>awesome [1.06e-01]</td>\n",
       "      <td>sack [6.57e-02]</td>\n",
       "      <td>educate [6.37e-02]</td>\n",
       "      <td>implications [1.35e-01]</td>\n",
       "      <td>dallas [6.73e-02]</td>\n",
       "      <td>everybody [6.01e-02]</td>\n",
       "      <td>winners [4.44e-02]</td>\n",
       "      <td>exhausted [1.23e-01]</td>\n",
       "      <td>charge [7.18e-02]</td>\n",
       "      <td>alert [1.12e-01]</td>\n",
       "      <td>macys [7.69e-02]</td>\n",
       "      <td>yesterday [8.62e-02]</td>\n",
       "      <td>local [4.61e-02]</td>\n",
       "      <td>main [7.41e-02]</td>\n",
       "      <td>produce [6.86e-02]</td>\n",
       "      <td>thankss [5.58e-02]</td>\n",
       "      <td>caught [1.30e-01]</td>\n",
       "      <td>musical [4.83e-02]</td>\n",
       "      <td>hood [8.01e-02]</td>\n",
       "      <td>empowering [5.90e-02]</td>\n",
       "      <td>submitted [1.43e-01]</td>\n",
       "      <td>cesar [8.09e-02]</td>\n",
       "      <td>awesome [1.29e-01]</td>\n",
       "      <td>awesome [9.56e-02]</td>\n",
       "      <td>lololololol [4.57e-02]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>macys [1.67e-01]</td>\n",
       "      <td>implications [1.41e-01]</td>\n",
       "      <td>memes [4.87e-02]</td>\n",
       "      <td>educate [1.56e-01]</td>\n",
       "      <td>working [1.68e-01]</td>\n",
       "      <td>educate [1.02e-01]</td>\n",
       "      <td>drake [6.52e-02]</td>\n",
       "      <td>hershey [5.89e-02]</td>\n",
       "      <td>jurassic [1.32e-01]</td>\n",
       "      <td>fe [6.70e-02]</td>\n",
       "      <td>powers [5.92e-02]</td>\n",
       "      <td>gusta [4.43e-02]</td>\n",
       "      <td>boarding [1.21e-01]</td>\n",
       "      <td>stores [6.87e-02]</td>\n",
       "      <td>gettin [1.12e-01]</td>\n",
       "      <td>working [7.18e-02]</td>\n",
       "      <td>despite [8.62e-02]</td>\n",
       "      <td>sees [4.58e-02]</td>\n",
       "      <td>ghost [7.30e-02]</td>\n",
       "      <td>euro [6.82e-02]</td>\n",
       "      <td>dvd [5.57e-02]</td>\n",
       "      <td>26 [1.30e-01]</td>\n",
       "      <td>metropolitan [4.82e-02]</td>\n",
       "      <td>diego [7.98e-02]</td>\n",
       "      <td>dubnation [5.85e-02]</td>\n",
       "      <td>scheduled [1.42e-01]</td>\n",
       "      <td>fiesta [8.08e-02]</td>\n",
       "      <td>rec [1.17e-01]</td>\n",
       "      <td>skies [9.37e-02]</td>\n",
       "      <td>hershey [4.54e-02]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>jurassic [1.64e-01]</td>\n",
       "      <td>jurassic [1.40e-01]</td>\n",
       "      <td>charge [4.80e-02]</td>\n",
       "      <td>macys [1.55e-01]</td>\n",
       "      <td>educate [1.66e-01]</td>\n",
       "      <td>cream [9.96e-02]</td>\n",
       "      <td>norma [6.39e-02]</td>\n",
       "      <td>skies [5.71e-02]</td>\n",
       "      <td>rec [1.21e-01]</td>\n",
       "      <td>strength [6.64e-02]</td>\n",
       "      <td>gin [5.86e-02]</td>\n",
       "      <td>cyclists [4.42e-02]</td>\n",
       "      <td>torn [1.20e-01]</td>\n",
       "      <td>empowering [6.70e-02]</td>\n",
       "      <td>golden [1.12e-01]</td>\n",
       "      <td>learning [7.16e-02]</td>\n",
       "      <td>hershey [8.13e-02]</td>\n",
       "      <td>wife [4.58e-02]</td>\n",
       "      <td>lifegoals [7.26e-02]</td>\n",
       "      <td>yg [6.82e-02]</td>\n",
       "      <td>toughest [5.57e-02]</td>\n",
       "      <td>arms [1.29e-01]</td>\n",
       "      <td>sensor [4.80e-02]</td>\n",
       "      <td>nighty [7.97e-02]</td>\n",
       "      <td>forsure [5.61e-02]</td>\n",
       "      <td>slap [1.40e-01]</td>\n",
       "      <td>tener [8.02e-02]</td>\n",
       "      <td>jurassic [1.15e-01]</td>\n",
       "      <td>empowering [9.05e-02]</td>\n",
       "      <td>sold [4.50e-02]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>empowering [1.55e-01]</td>\n",
       "      <td>working [1.31e-01]</td>\n",
       "      <td>loveher [4.61e-02]</td>\n",
       "      <td>jurassic [1.52e-01]</td>\n",
       "      <td>jurassic [1.66e-01]</td>\n",
       "      <td>chocolate [9.96e-02]</td>\n",
       "      <td>memes [6.10e-02]</td>\n",
       "      <td>learning [5.69e-02]</td>\n",
       "      <td>sa [1.20e-01]</td>\n",
       "      <td>banner [6.61e-02]</td>\n",
       "      <td>strawberries [5.85e-02]</td>\n",
       "      <td>sphere [4.34e-02]</td>\n",
       "      <td>sex [1.17e-01]</td>\n",
       "      <td>despite [6.29e-02]</td>\n",
       "      <td>2010 [1.11e-01]</td>\n",
       "      <td>rec [6.79e-02]</td>\n",
       "      <td>jurassic [7.85e-02]</td>\n",
       "      <td>urges [4.51e-02]</td>\n",
       "      <td>post [7.22e-02]</td>\n",
       "      <td>iggy [6.80e-02]</td>\n",
       "      <td>gods [5.56e-02]</td>\n",
       "      <td>daddy [1.28e-01]</td>\n",
       "      <td>poking [4.76e-02]</td>\n",
       "      <td>caught [7.89e-02]</td>\n",
       "      <td>kisses [5.51e-02]</td>\n",
       "      <td>helps [1.39e-01]</td>\n",
       "      <td>patience [7.95e-02]</td>\n",
       "      <td>educate [1.12e-01]</td>\n",
       "      <td>stores [8.99e-02]</td>\n",
       "      <td>chloeonvine [4.45e-02]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>rec [1.50e-01]</td>\n",
       "      <td>batman [1.31e-01]</td>\n",
       "      <td>conclusions [4.55e-02]</td>\n",
       "      <td>stores [1.52e-01]</td>\n",
       "      <td>hershey [1.65e-01]</td>\n",
       "      <td>hershey [9.92e-02]</td>\n",
       "      <td>resolved [6.08e-02]</td>\n",
       "      <td>fashionista [5.59e-02]</td>\n",
       "      <td>learning [1.20e-01]</td>\n",
       "      <td>slater [6.60e-02]</td>\n",
       "      <td>1am [5.78e-02]</td>\n",
       "      <td>lalife [4.28e-02]</td>\n",
       "      <td>extremely [1.17e-01]</td>\n",
       "      <td>cavs [6.24e-02]</td>\n",
       "      <td>ac [1.11e-01]</td>\n",
       "      <td>charge [6.78e-02]</td>\n",
       "      <td>sa [7.75e-02]</td>\n",
       "      <td>acquisition [4.45e-02]</td>\n",
       "      <td>doors [7.20e-02]</td>\n",
       "      <td>awaited [6.79e-02]</td>\n",
       "      <td>locked [5.54e-02]</td>\n",
       "      <td>refs [1.27e-01]</td>\n",
       "      <td>architect [4.73e-02]</td>\n",
       "      <td>beyonce [7.87e-02]</td>\n",
       "      <td>aubrey [5.45e-02]</td>\n",
       "      <td>ipod [1.39e-01]</td>\n",
       "      <td>general [7.95e-02]</td>\n",
       "      <td>kisses [1.10e-01]</td>\n",
       "      <td>sa [8.78e-02]</td>\n",
       "      <td>pushups [4.35e-02]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>jcolenc [1.49e-01]</td>\n",
       "      <td>educate [1.31e-01]</td>\n",
       "      <td>hershey [4.52e-02]</td>\n",
       "      <td>kisses [1.47e-01]</td>\n",
       "      <td>learning [1.60e-01]</td>\n",
       "      <td>kisses [9.76e-02]</td>\n",
       "      <td>learning [5.83e-02]</td>\n",
       "      <td>mocked [5.19e-02]</td>\n",
       "      <td>sold [1.19e-01]</td>\n",
       "      <td>blitz [6.60e-02]</td>\n",
       "      <td>headache [5.77e-02]</td>\n",
       "      <td>colt [4.20e-02]</td>\n",
       "      <td>mo [1.17e-01]</td>\n",
       "      <td>picture [6.15e-02]</td>\n",
       "      <td>tragedy [1.10e-01]</td>\n",
       "      <td>yum [6.76e-02]</td>\n",
       "      <td>jcolenc [7.66e-02]</td>\n",
       "      <td>website [4.36e-02]</td>\n",
       "      <td>pc [7.18e-02]</td>\n",
       "      <td>attn [6.77e-02]</td>\n",
       "      <td>murray [5.46e-02]</td>\n",
       "      <td>thankfully [1.26e-01]</td>\n",
       "      <td>139 [4.55e-02]</td>\n",
       "      <td>porn [7.86e-02]</td>\n",
       "      <td>cavs [5.39e-02]</td>\n",
       "      <td>whats [1.38e-01]</td>\n",
       "      <td>dawn [7.85e-02]</td>\n",
       "      <td>stores [1.09e-01]</td>\n",
       "      <td>hershey [8.48e-02]</td>\n",
       "      <td>ministry [4.34e-02]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>batman [1.44e-01]</td>\n",
       "      <td>kisses [1.30e-01]</td>\n",
       "      <td>despite [4.52e-02]</td>\n",
       "      <td>rec [1.46e-01]</td>\n",
       "      <td>online [1.57e-01]</td>\n",
       "      <td>working [9.38e-02]</td>\n",
       "      <td>relevance [5.81e-02]</td>\n",
       "      <td>nip [5.17e-02]</td>\n",
       "      <td>empowering [1.18e-01]</td>\n",
       "      <td>pie [6.58e-02]</td>\n",
       "      <td>cryin [5.66e-02]</td>\n",
       "      <td>neglect [4.16e-02]</td>\n",
       "      <td>2011 [1.17e-01]</td>\n",
       "      <td>derpey [5.96e-02]</td>\n",
       "      <td>popcorn [1.09e-01]</td>\n",
       "      <td>empowering [6.73e-02]</td>\n",
       "      <td>working [7.52e-02]</td>\n",
       "      <td>twenty [4.36e-02]</td>\n",
       "      <td>los [7.14e-02]</td>\n",
       "      <td>brag [6.72e-02]</td>\n",
       "      <td>fields [5.45e-02]</td>\n",
       "      <td>wasnt [1.26e-01]</td>\n",
       "      <td>huddle [4.49e-02]</td>\n",
       "      <td>portugal [7.85e-02]</td>\n",
       "      <td>teenagernotes [5.25e-02]</td>\n",
       "      <td>patience [1.38e-01]</td>\n",
       "      <td>al [7.82e-02]</td>\n",
       "      <td>macys [1.07e-01]</td>\n",
       "      <td>jurassic [8.11e-02]</td>\n",
       "      <td>fuq [4.33e-02]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>implications [1.43e-01]</td>\n",
       "      <td>ice [1.22e-01]</td>\n",
       "      <td>stability [4.51e-02]</td>\n",
       "      <td>implications [1.42e-01]</td>\n",
       "      <td>iphone [1.54e-01]</td>\n",
       "      <td>implications [9.24e-02]</td>\n",
       "      <td>picky [5.73e-02]</td>\n",
       "      <td>hearst [5.17e-02]</td>\n",
       "      <td>macys [1.17e-01]</td>\n",
       "      <td>ortiz [6.52e-02]</td>\n",
       "      <td>shin [5.66e-02]</td>\n",
       "      <td>swanky [4.16e-02]</td>\n",
       "      <td>kit [1.16e-01]</td>\n",
       "      <td>yesterday [5.91e-02]</td>\n",
       "      <td>none [1.09e-01]</td>\n",
       "      <td>jurassic [6.70e-02]</td>\n",
       "      <td>case [7.21e-02]</td>\n",
       "      <td>era [4.34e-02]</td>\n",
       "      <td>homes [7.11e-02]</td>\n",
       "      <td>cu [6.71e-02]</td>\n",
       "      <td>101 [5.45e-02]</td>\n",
       "      <td>sex [1.26e-01]</td>\n",
       "      <td>handbook [4.48e-02]</td>\n",
       "      <td>arms [7.84e-02]</td>\n",
       "      <td>foxsports [5.22e-02]</td>\n",
       "      <td>refs [1.36e-01]</td>\n",
       "      <td>struggling [7.79e-02]</td>\n",
       "      <td>skies [1.01e-01]</td>\n",
       "      <td>online [7.90e-02]</td>\n",
       "      <td>paved [4.29e-02]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>chocolate [1.33e-01]</td>\n",
       "      <td>stores [1.21e-01]</td>\n",
       "      <td>moi [4.46e-02]</td>\n",
       "      <td>online [1.33e-01]</td>\n",
       "      <td>macys [1.49e-01]</td>\n",
       "      <td>batman [9.18e-02]</td>\n",
       "      <td>hershey [5.62e-02]</td>\n",
       "      <td>nerf [5.10e-02]</td>\n",
       "      <td>kisses [1.16e-01]</td>\n",
       "      <td>torn [6.49e-02]</td>\n",
       "      <td>bitching [5.61e-02]</td>\n",
       "      <td>moan [4.05e-02]</td>\n",
       "      <td>revenge [1.16e-01]</td>\n",
       "      <td>implications [5.90e-02]</td>\n",
       "      <td>highly [1.08e-01]</td>\n",
       "      <td>stores [6.66e-02]</td>\n",
       "      <td>manti [7.16e-02]</td>\n",
       "      <td>quoteoftheday [4.32e-02]</td>\n",
       "      <td>whats [7.10e-02]</td>\n",
       "      <td>nighty [6.71e-02]</td>\n",
       "      <td>ways [5.41e-02]</td>\n",
       "      <td>golden [1.26e-01]</td>\n",
       "      <td>hailing [4.48e-02]</td>\n",
       "      <td>kit [7.83e-02]</td>\n",
       "      <td>tigerwoods [5.13e-02]</td>\n",
       "      <td>frustrated [1.36e-01]</td>\n",
       "      <td>fit [7.78e-02]</td>\n",
       "      <td>learning [1.00e-01]</td>\n",
       "      <td>macys [7.74e-02]</td>\n",
       "      <td>tiempo [4.28e-02]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>manti [1.31e-01]</td>\n",
       "      <td>manti [1.14e-01]</td>\n",
       "      <td>proclaimed [4.39e-02]</td>\n",
       "      <td>working [1.33e-01]</td>\n",
       "      <td>sa [1.49e-01]</td>\n",
       "      <td>rec [8.74e-02]</td>\n",
       "      <td>xmas [5.50e-02]</td>\n",
       "      <td>caveman [5.07e-02]</td>\n",
       "      <td>stores [1.14e-01]</td>\n",
       "      <td>dealers [6.45e-02]</td>\n",
       "      <td>copied [5.59e-02]</td>\n",
       "      <td>artistry [4.03e-02]</td>\n",
       "      <td>knowing [1.15e-01]</td>\n",
       "      <td>jurassic [5.85e-02]</td>\n",
       "      <td>preview [1.08e-01]</td>\n",
       "      <td>picture [6.65e-02]</td>\n",
       "      <td>eating [7.13e-02]</td>\n",
       "      <td>whining [4.32e-02]</td>\n",
       "      <td>says [7.04e-02]</td>\n",
       "      <td>______ [6.67e-02]</td>\n",
       "      <td>30am [5.40e-02]</td>\n",
       "      <td>alert [1.26e-01]</td>\n",
       "      <td>cd [4.46e-02]</td>\n",
       "      <td>bumps [7.77e-02]</td>\n",
       "      <td>ov [5.06e-02]</td>\n",
       "      <td>pig [1.36e-01]</td>\n",
       "      <td>whats [7.76e-02]</td>\n",
       "      <td>working [9.96e-02]</td>\n",
       "      <td>sold [7.50e-02]</td>\n",
       "      <td>picture [4.27e-02]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         0                        1                       2   \\\n",
       "0          needs [3.91e-01]         needs [3.75e-01]       sunset [1.74e-01]   \n",
       "1          ranch [3.50e-01]        mother [3.39e-01]        needs [1.72e-01]   \n",
       "2         sunset [3.45e-01]        sunset [3.39e-01]       mother [1.70e-01]   \n",
       "3         mother [3.44e-01]         ranch [3.27e-01]        ranch [1.64e-01]   \n",
       "4           kiss [3.13e-01]          kiss [3.14e-01]         kiss [1.52e-01]   \n",
       "5      yesterday [2.15e-01]       hershey [1.84e-01]   empowering [7.15e-02]   \n",
       "6        working [1.88e-01]       picture [1.71e-01]           sa [6.51e-02]   \n",
       "7        picture [1.81e-01]     yesterday [1.67e-01]      educate [6.51e-02]   \n",
       "8        hershey [1.79e-01]       awesome [1.60e-01]      picture [5.16e-02]   \n",
       "9        awesome [1.75e-01]    empowering [1.48e-01]      awesome [5.11e-02]   \n",
       "10       educate [1.69e-01]         macys [1.42e-01]     longtime [4.98e-02]   \n",
       "11         macys [1.67e-01]  implications [1.41e-01]        memes [4.87e-02]   \n",
       "12      jurassic [1.64e-01]      jurassic [1.40e-01]       charge [4.80e-02]   \n",
       "13    empowering [1.55e-01]       working [1.31e-01]      loveher [4.61e-02]   \n",
       "14           rec [1.50e-01]        batman [1.31e-01]  conclusions [4.55e-02]   \n",
       "15       jcolenc [1.49e-01]       educate [1.31e-01]      hershey [4.52e-02]   \n",
       "16        batman [1.44e-01]        kisses [1.30e-01]      despite [4.52e-02]   \n",
       "17  implications [1.43e-01]           ice [1.22e-01]    stability [4.51e-02]   \n",
       "18     chocolate [1.33e-01]        stores [1.21e-01]          moi [4.46e-02]   \n",
       "19         manti [1.31e-01]         manti [1.14e-01]   proclaimed [4.39e-02]   \n",
       "\n",
       "                         3                        4                        5   \\\n",
       "0          needs [3.29e-01]         needs [4.08e-01]         ranch [2.62e-01]   \n",
       "1         sunset [3.18e-01]         ranch [4.05e-01]        mother [2.60e-01]   \n",
       "2          ranch [3.14e-01]        mother [3.98e-01]         needs [2.55e-01]   \n",
       "3         mother [3.07e-01]        sunset [3.78e-01]        sunset [2.30e-01]   \n",
       "4           kiss [3.00e-01]          kiss [3.60e-01]          kiss [2.00e-01]   \n",
       "5     empowering [1.86e-01]       awesome [2.21e-01]       picture [1.19e-01]   \n",
       "6      yesterday [1.76e-01]       picture [2.09e-01]      jurassic [1.18e-01]   \n",
       "7        awesome [1.71e-01]     yesterday [1.90e-01]     yesterday [1.14e-01]   \n",
       "8        hershey [1.64e-01]        kisses [1.77e-01]        stores [1.12e-01]   \n",
       "9             sa [1.60e-01]  implications [1.77e-01]      learning [1.09e-01]   \n",
       "10       picture [1.59e-01]    empowering [1.71e-01]       awesome [1.06e-01]   \n",
       "11       educate [1.56e-01]       working [1.68e-01]       educate [1.02e-01]   \n",
       "12         macys [1.55e-01]       educate [1.66e-01]         cream [9.96e-02]   \n",
       "13      jurassic [1.52e-01]      jurassic [1.66e-01]     chocolate [9.96e-02]   \n",
       "14        stores [1.52e-01]       hershey [1.65e-01]       hershey [9.92e-02]   \n",
       "15        kisses [1.47e-01]      learning [1.60e-01]        kisses [9.76e-02]   \n",
       "16           rec [1.46e-01]        online [1.57e-01]       working [9.38e-02]   \n",
       "17  implications [1.42e-01]        iphone [1.54e-01]  implications [9.24e-02]   \n",
       "18        online [1.33e-01]         macys [1.49e-01]        batman [9.18e-02]   \n",
       "19       working [1.33e-01]            sa [1.49e-01]           rec [8.74e-02]   \n",
       "\n",
       "                         6                        7                        8   \\\n",
       "0          needs [2.65e-01]        sunset [2.15e-01]         needs [3.84e-01]   \n",
       "1          ranch [2.65e-01]         ranch [2.10e-01]         ranch [3.68e-01]   \n",
       "2         mother [2.62e-01]         needs [2.09e-01]        mother [3.62e-01]   \n",
       "3         sunset [2.49e-01]        mother [1.98e-01]        sunset [3.22e-01]   \n",
       "4           kiss [2.39e-01]          kiss [1.71e-01]          kiss [3.16e-01]   \n",
       "5        picture [9.85e-02]       picture [8.51e-02]       picture [1.76e-01]   \n",
       "6          manti [9.32e-02]    empowering [7.50e-02]     yesterday [1.51e-01]   \n",
       "7        educate [7.47e-02]  implications [7.06e-02]       hershey [1.49e-01]   \n",
       "8   stormtrooper [7.24e-02]        online [6.61e-02]       educate [1.39e-01]   \n",
       "9      yesterday [6.85e-02]         manti [6.58e-02]       awesome [1.39e-01]   \n",
       "10          sack [6.57e-02]       educate [6.37e-02]  implications [1.35e-01]   \n",
       "11         drake [6.52e-02]       hershey [5.89e-02]      jurassic [1.32e-01]   \n",
       "12         norma [6.39e-02]         skies [5.71e-02]           rec [1.21e-01]   \n",
       "13         memes [6.10e-02]      learning [5.69e-02]            sa [1.20e-01]   \n",
       "14      resolved [6.08e-02]   fashionista [5.59e-02]      learning [1.20e-01]   \n",
       "15      learning [5.83e-02]        mocked [5.19e-02]          sold [1.19e-01]   \n",
       "16     relevance [5.81e-02]           nip [5.17e-02]    empowering [1.18e-01]   \n",
       "17         picky [5.73e-02]        hearst [5.17e-02]         macys [1.17e-01]   \n",
       "18       hershey [5.62e-02]          nerf [5.10e-02]        kisses [1.16e-01]   \n",
       "19          xmas [5.50e-02]       caveman [5.07e-02]        stores [1.14e-01]   \n",
       "\n",
       "                      9                        10                      11  \\\n",
       "0   argentina [8.58e-02]       romance [6.62e-02]     sneezing [5.39e-02]   \n",
       "1      hmmmmm [8.36e-02]      cleaning [6.57e-02]    resisting [5.08e-02]   \n",
       "2     removed [7.51e-02]           nut [6.44e-02]        cielo [4.80e-02]   \n",
       "3     praying [7.18e-02]       gosling [6.38e-02]          kik [4.72e-02]   \n",
       "4         dre [7.17e-02]       nutella [6.36e-02]   indictment [4.62e-02]   \n",
       "5    zeppelin [7.10e-02]           bbq [6.35e-02]        cedar [4.61e-02]   \n",
       "6        lead [7.02e-02]     backwards [6.23e-02]  registering [4.56e-02]   \n",
       "7        dgaf [6.98e-02]           due [6.18e-02]  desperation [4.54e-02]   \n",
       "8         bat [6.85e-02]           dry [6.06e-02]     werewolf [4.50e-02]   \n",
       "9          ta [6.76e-02]           o_o [6.05e-02]    prisoners [4.48e-02]   \n",
       "10     dallas [6.73e-02]     everybody [6.01e-02]      winners [4.44e-02]   \n",
       "11         fe [6.70e-02]        powers [5.92e-02]        gusta [4.43e-02]   \n",
       "12   strength [6.64e-02]           gin [5.86e-02]     cyclists [4.42e-02]   \n",
       "13     banner [6.61e-02]  strawberries [5.85e-02]       sphere [4.34e-02]   \n",
       "14     slater [6.60e-02]           1am [5.78e-02]       lalife [4.28e-02]   \n",
       "15      blitz [6.60e-02]      headache [5.77e-02]         colt [4.20e-02]   \n",
       "16        pie [6.58e-02]         cryin [5.66e-02]      neglect [4.16e-02]   \n",
       "17      ortiz [6.52e-02]          shin [5.66e-02]       swanky [4.16e-02]   \n",
       "18       torn [6.49e-02]      bitching [5.61e-02]         moan [4.05e-02]   \n",
       "19    dealers [6.45e-02]        copied [5.59e-02]     artistry [4.03e-02]   \n",
       "\n",
       "                        12                       13                      14  \\\n",
       "0        ahhhhh [1.36e-01]         needs [2.09e-01]        heavy [1.31e-01]   \n",
       "1   appreciated [1.31e-01]         ranch [2.07e-01]        outta [1.30e-01]   \n",
       "2      patience [1.28e-01]        mother [2.02e-01]    billboard [1.21e-01]   \n",
       "3       sundays [1.27e-01]          kiss [1.92e-01]          kit [1.17e-01]   \n",
       "4     billboard [1.26e-01]        sunset [1.83e-01]     patience [1.14e-01]   \n",
       "5        cooler [1.25e-01]       educate [1.09e-01]         hood [1.13e-01]   \n",
       "6            sc [1.24e-01]       awesome [8.49e-02]        heads [1.13e-01]   \n",
       "7    frustrated [1.24e-01]       hershey [7.67e-02]        helps [1.13e-01]   \n",
       "8         alert [1.24e-01]       jcolenc [7.32e-02]  intelligent [1.13e-01]   \n",
       "9          root [1.23e-01]         manti [7.24e-02]          lee [1.12e-01]   \n",
       "10    exhausted [1.23e-01]        charge [7.18e-02]        alert [1.12e-01]   \n",
       "11     boarding [1.21e-01]        stores [6.87e-02]       gettin [1.12e-01]   \n",
       "12         torn [1.20e-01]    empowering [6.70e-02]       golden [1.12e-01]   \n",
       "13          sex [1.17e-01]       despite [6.29e-02]         2010 [1.11e-01]   \n",
       "14    extremely [1.17e-01]          cavs [6.24e-02]           ac [1.11e-01]   \n",
       "15           mo [1.17e-01]       picture [6.15e-02]      tragedy [1.10e-01]   \n",
       "16         2011 [1.17e-01]        derpey [5.96e-02]      popcorn [1.09e-01]   \n",
       "17          kit [1.16e-01]     yesterday [5.91e-02]         none [1.09e-01]   \n",
       "18      revenge [1.16e-01]  implications [5.90e-02]       highly [1.08e-01]   \n",
       "19      knowing [1.15e-01]      jurassic [5.85e-02]      preview [1.08e-01]   \n",
       "\n",
       "                         15                     16                        17  \\\n",
       "0          needs [2.40e-01]       needs [2.42e-01]       sourcing [5.22e-02]   \n",
       "1          ranch [2.23e-01]      sunset [2.21e-01]            fcc [5.06e-02]   \n",
       "2         mother [2.08e-01]       ranch [2.20e-01]           amar [4.90e-02]   \n",
       "3         sunset [1.98e-01]      mother [2.10e-01]           cafe [4.81e-02]   \n",
       "4           kiss [1.76e-01]        kiss [2.08e-01]          nears [4.72e-02]   \n",
       "5      yesterday [9.90e-02]     educate [1.02e-01]        pelican [4.72e-02]   \n",
       "6        hershey [9.83e-02]     picture [9.78e-02]   demonstrates [4.67e-02]   \n",
       "7         kisses [9.09e-02]      iphone [9.03e-02]          biden [4.64e-02]   \n",
       "8   implications [8.22e-02]     awesome [8.95e-02]         dayton [4.64e-02]   \n",
       "9             sa [8.07e-02]  empowering [8.86e-02]            tap [4.64e-02]   \n",
       "10         macys [7.69e-02]   yesterday [8.62e-02]          local [4.61e-02]   \n",
       "11       working [7.18e-02]     despite [8.62e-02]           sees [4.58e-02]   \n",
       "12      learning [7.16e-02]     hershey [8.13e-02]           wife [4.58e-02]   \n",
       "13           rec [6.79e-02]    jurassic [7.85e-02]          urges [4.51e-02]   \n",
       "14        charge [6.78e-02]          sa [7.75e-02]    acquisition [4.45e-02]   \n",
       "15           yum [6.76e-02]     jcolenc [7.66e-02]        website [4.36e-02]   \n",
       "16    empowering [6.73e-02]     working [7.52e-02]         twenty [4.36e-02]   \n",
       "17      jurassic [6.70e-02]        case [7.21e-02]            era [4.34e-02]   \n",
       "18        stores [6.66e-02]       manti [7.16e-02]  quoteoftheday [4.32e-02]   \n",
       "19       picture [6.65e-02]      eating [7.13e-02]        whining [4.32e-02]   \n",
       "\n",
       "                       18                     19                    20  \\\n",
       "0         hair [8.45e-02]    smallest [7.48e-02]       jerk [7.31e-02]   \n",
       "1     research [8.12e-02]        left [7.29e-02]  favorites [6.46e-02]   \n",
       "2     facebook [7.75e-02]      shazam [7.26e-02]     ahhhhh [6.30e-02]   \n",
       "3         cafe [7.75e-02]      dipped [7.03e-02]    grandpa [5.95e-02]   \n",
       "4        loves [7.72e-02]          da [7.02e-02]       hasn [5.88e-02]   \n",
       "5   foursquare [7.67e-02]        bali [6.99e-02]       gigs [5.85e-02]   \n",
       "6        hopes [7.63e-02]   candidate [6.94e-02]    silicon [5.73e-02]   \n",
       "7         lady [7.56e-02]  comparable [6.90e-02]    naughty [5.69e-02]   \n",
       "8    americano [7.55e-02]          27 [6.88e-02]  thousands [5.66e-02]   \n",
       "9      landing [7.42e-02]       shirt [6.88e-02]      keeps [5.65e-02]   \n",
       "10        main [7.41e-02]     produce [6.86e-02]    thankss [5.58e-02]   \n",
       "11       ghost [7.30e-02]        euro [6.82e-02]        dvd [5.57e-02]   \n",
       "12   lifegoals [7.26e-02]          yg [6.82e-02]   toughest [5.57e-02]   \n",
       "13        post [7.22e-02]        iggy [6.80e-02]       gods [5.56e-02]   \n",
       "14       doors [7.20e-02]     awaited [6.79e-02]     locked [5.54e-02]   \n",
       "15          pc [7.18e-02]        attn [6.77e-02]     murray [5.46e-02]   \n",
       "16         los [7.14e-02]        brag [6.72e-02]     fields [5.45e-02]   \n",
       "17       homes [7.11e-02]          cu [6.71e-02]        101 [5.45e-02]   \n",
       "18       whats [7.10e-02]      nighty [6.71e-02]       ways [5.41e-02]   \n",
       "19        says [7.04e-02]      ______ [6.67e-02]       30am [5.40e-02]   \n",
       "\n",
       "                       21                       22                   23  \\\n",
       "0         2010 [1.43e-01]      cyclists [6.14e-02]   forward [9.12e-02]   \n",
       "1       killed [1.41e-01]       threads [5.74e-02]     shows [9.12e-02]   \n",
       "2     horrible [1.41e-01]    slideshare [5.40e-02]      post [9.01e-02]   \n",
       "3       unless [1.40e-01]         sochi [5.33e-02]        lt [8.91e-02]   \n",
       "4         roll [1.39e-01]          kahn [5.20e-02]    holler [8.76e-02]   \n",
       "5         iggy [1.35e-01]       compact [5.01e-02]       era [8.64e-02]   \n",
       "6     contract [1.33e-01]           nyc [4.99e-02]       sad [8.50e-02]   \n",
       "7        heads [1.33e-01]      engaging [4.90e-02]      star [8.22e-02]   \n",
       "8        helps [1.31e-01]          pena [4.87e-02]     topic [8.10e-02]   \n",
       "9        ended [1.31e-01]       vaguely [4.84e-02]  starwars [8.04e-02]   \n",
       "10      caught [1.30e-01]       musical [4.83e-02]      hood [8.01e-02]   \n",
       "11          26 [1.30e-01]  metropolitan [4.82e-02]     diego [7.98e-02]   \n",
       "12        arms [1.29e-01]        sensor [4.80e-02]    nighty [7.97e-02]   \n",
       "13       daddy [1.28e-01]        poking [4.76e-02]    caught [7.89e-02]   \n",
       "14        refs [1.27e-01]     architect [4.73e-02]   beyonce [7.87e-02]   \n",
       "15  thankfully [1.26e-01]           139 [4.55e-02]      porn [7.86e-02]   \n",
       "16       wasnt [1.26e-01]        huddle [4.49e-02]  portugal [7.85e-02]   \n",
       "17         sex [1.26e-01]      handbook [4.48e-02]      arms [7.84e-02]   \n",
       "18      golden [1.26e-01]       hailing [4.48e-02]       kit [7.83e-02]   \n",
       "19       alert [1.26e-01]            cd [4.46e-02]     bumps [7.77e-02]   \n",
       "\n",
       "                          24                      25                      26  \\\n",
       "0           needs [2.56e-01]         lack [1.59e-01]         dias [9.00e-02]   \n",
       "1           ranch [2.49e-01]     contract [1.55e-01]       artist [8.95e-02]   \n",
       "2          mother [2.44e-01]         2010 [1.52e-01]          kit [8.66e-02]   \n",
       "3          sunset [2.19e-01]   thankfully [1.51e-01]       frozen [8.61e-02]   \n",
       "4            kiss [2.17e-01]       cooler [1.50e-01]        grand [8.47e-02]   \n",
       "5         picture [8.20e-02]     industry [1.49e-01]         haha [8.46e-02]   \n",
       "6         hershey [7.46e-02]          bat [1.48e-01]           lo [8.37e-02]   \n",
       "7         educate [6.39e-02]      serious [1.44e-01]        total [8.35e-02]   \n",
       "8              sa [6.33e-02]          9am [1.44e-01]       flower [8.18e-02]   \n",
       "9          flames [6.29e-02]  complaining [1.44e-01]  appreciated [8.13e-02]   \n",
       "10     empowering [5.90e-02]    submitted [1.43e-01]        cesar [8.09e-02]   \n",
       "11      dubnation [5.85e-02]    scheduled [1.42e-01]       fiesta [8.08e-02]   \n",
       "12        forsure [5.61e-02]         slap [1.40e-01]        tener [8.02e-02]   \n",
       "13         kisses [5.51e-02]        helps [1.39e-01]     patience [7.95e-02]   \n",
       "14         aubrey [5.45e-02]         ipod [1.39e-01]      general [7.95e-02]   \n",
       "15           cavs [5.39e-02]        whats [1.38e-01]         dawn [7.85e-02]   \n",
       "16  teenagernotes [5.25e-02]     patience [1.38e-01]           al [7.82e-02]   \n",
       "17      foxsports [5.22e-02]         refs [1.36e-01]   struggling [7.79e-02]   \n",
       "18     tigerwoods [5.13e-02]   frustrated [1.36e-01]          fit [7.78e-02]   \n",
       "19             ov [5.06e-02]          pig [1.36e-01]        whats [7.76e-02]   \n",
       "\n",
       "                         27                       28  \\\n",
       "0          needs [2.87e-01]         ranch [2.77e-01]   \n",
       "1         mother [2.86e-01]         needs [2.39e-01]   \n",
       "2          ranch [2.70e-01]        mother [2.34e-01]   \n",
       "3           kiss [2.69e-01]        sunset [2.33e-01]   \n",
       "4         sunset [2.57e-01]          kiss [2.11e-01]   \n",
       "5        hershey [1.60e-01]  implications [1.18e-01]   \n",
       "6     empowering [1.46e-01]       picture [1.17e-01]   \n",
       "7   implications [1.40e-01]      students [1.15e-01]   \n",
       "8        picture [1.30e-01]       educate [1.02e-01]   \n",
       "9      yesterday [1.30e-01]      learning [1.01e-01]   \n",
       "10       awesome [1.29e-01]       awesome [9.56e-02]   \n",
       "11           rec [1.17e-01]         skies [9.37e-02]   \n",
       "12      jurassic [1.15e-01]    empowering [9.05e-02]   \n",
       "13       educate [1.12e-01]        stores [8.99e-02]   \n",
       "14        kisses [1.10e-01]            sa [8.78e-02]   \n",
       "15        stores [1.09e-01]       hershey [8.48e-02]   \n",
       "16         macys [1.07e-01]      jurassic [8.11e-02]   \n",
       "17         skies [1.01e-01]        online [7.90e-02]   \n",
       "18      learning [1.00e-01]         macys [7.74e-02]   \n",
       "19       working [9.96e-02]          sold [7.50e-02]   \n",
       "\n",
       "                            29  \n",
       "0          jurassic [5.62e-02]  \n",
       "1               rec [5.62e-02]  \n",
       "2              bird [5.06e-02]  \n",
       "3   highschoolsuckz [5.02e-02]  \n",
       "4   conspiracyimage [4.80e-02]  \n",
       "5         shawshank [4.78e-02]  \n",
       "6             upppp [4.75e-02]  \n",
       "7             waaay [4.70e-02]  \n",
       "8            iphone [4.69e-02]  \n",
       "9           scarred [4.64e-02]  \n",
       "10      lololololol [4.57e-02]  \n",
       "11          hershey [4.54e-02]  \n",
       "12             sold [4.50e-02]  \n",
       "13      chloeonvine [4.45e-02]  \n",
       "14          pushups [4.35e-02]  \n",
       "15         ministry [4.34e-02]  \n",
       "16              fuq [4.33e-02]  \n",
       "17            paved [4.29e-02]  \n",
       "18           tiempo [4.28e-02]  \n",
       "19          picture [4.27e-02]  "
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = []\n",
    "for i in range(tw.shape[1]):\n",
    "    top_fts_idx = tw[:,i].argsort()[::-1][:20]\n",
    "    words = [\"{} [{:.2e}]\".format(word, weight) for word, weight in zip(twitter.features[top_fts_idx], tw[:,i][top_fts_idx])]\n",
    "    t.append(words)\n",
    "pd.DataFrame(t).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
